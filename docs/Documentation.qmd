---
title: "Clustering Data and Visualizing in 2D"
toc: true
toc-depth: 2
number-sections: true
bibliography: references.bib

author:   
  - Mate Balogh
  - Daniel Korchmaros
  - Bishara Hodali
  - Barnabas Hellman

project:
  header-includes:
    - \usepackage{titlesec}
    - \titlespacing*{\subsection}{0pt}{1.5em}{1em}

format:
  html:
    output-file: index.html
#  pdf:
#    latex-engine: xelatex
#    documentclass: article
#    fontsize: 11pt
#    geometry: margin=1in
#    linestretch: 1
#    cite-method: biblatex
---

\newpage
## Introduction

In this project, titled as *Clustering Data and Visualizing in 2D*, we aim to group similar data points of a dataset together and then visualize *where* and *how* those groups separate from each other. From a theoretical point of view, we plan to explore concepts such as:

- General Data Science pipeline
- Unsupervised learning
- Representation learning
- Neural networks,

while from the practical aspect, we expect to get hands on experience in using:

- *K-means*
- *Self-Organizing Map (SOM)* algorithms

The diagram below provides a high level view on the topics covered in the project and how they build upon each other. As included in the diagram, the use of 2D visualizations are considered as the primary tool to **explore**, **understand** and **explain** **patterns** in the data. Besides, expectations are we will get a closer look at how unsupervised and representation learning works along with how we can visualize high-dimensional data, in a way that is interpretable for the human eye, using SOMs.

![Bird's eye view of the project.](./images/overview.png){#fig-intro}




\newpage
## Background

### General Data Science Pipeline

@fig-background-01 provides a general view on the *Data Science* workflow, while @fig-background-02 displays a broader view of it. Elaboration of the main steps are as follows.

**Define and understand the problem**

The problem needs to be clearly defined and then turned into a Data Science task, so clear steps can be defined on how to solve it.

**Data collection**

In this step, it is important to collect the data that is suitable for the job. It is a good idea to collect more data than you need as data sets can be incomplete and dirty. Although, it is very important to highlight that the *quality* of the data is more important than the *quantity*.

**Data cleaning and preprocessing**

Raw data can include missing entries, duplicates, extreme outliers, to name a few characteristics that needs handling. Un- or improperly processed data will lead to bad models.

**Data exploration**

In this phase—often referred to as *Exploratory Data Analysis*—characteristics of the dataset are summarized and visualized. It helps in understanding what the collected data tells us: answer questions right away, see patterns or anomalies that could guide in building a better model.

**Model building**

In this part, the preprocessed data is used to train a model that discovers hidden patterns or learn from it in order to make predictions for the future.

**Evaluation**

The trained model is evaluated on never before seen samples that we refer to as the test (or holdout) set, by which we can verify how well the model generalizes.

**Deployment and refinement**

In this stage, models get deployed, observed and refinements are initiated to start the cycle again.

![General pipeline of Data Science. Figure by @datascience_pm_workflow.](images/ds-pipeline-1.png){#fig-background-01 width=80%}

![Graphic summary of the Data Science pipeline. Figure by @illinois_exploration.](images/ds-pipeline-2.png){#fig-background-02}



### Supervision in Machine Learning

<!-- Models usually fall into two categories: *Supervised* and *Unsupervised*. Supervised models have a known outcome, it has labeled data. For example, a model trained on spam emails would include examples of both spam and non-spam emails, from which it „learns” what a spam email looks like. Unsupervised models, on the other hand, don’t have such labels, they compare patterns and trends in data. For example, if you wanted to know the shopping habits of your customers based on their preferences. Unsupervised learning can be challenging as its output can be vague and inconcrete which can lead to bias. It's evaluation is completely different than that of supervised approaches. Still unsupervised learning has the advantage to spot hiddenor previously not seen paterns, that supervised models cannot recognize unless specifically trained to do so. -->

#### Supervised

In supervised feature learning, the input data is labeled, meaning data given to the model includes input-label pairs. This resorts in a representation with high label prediction accuracy. Examples include supervised neural networks, multilayer perceptrons, and dictionary learning.

#### Unsupervised

Unsupervised feature learning uses unlabelled input data, it learns patterns by analyzing the relationship between data points. Examples include Clustering (K-means), ICA and matrix factorization.

#### Other methods

In semi-supervised learning, a model is trained on data related to one class, or some of the classes. Real life scenarios include anomaly detection where there is sufficient training data for normal events while anomalous ones occur rarely, expensive to acquire, or they evolve in a way that only deviations from the normal boundary can successfully detect them.

In self-supervised learning, features are learned using unlabelled data like unsupervised learning, however input-label pairs are constructed from each data point, enabling learning the structure of the data through supervised methods. Examples include word embeddings and autoencoders.

### Representation Learning

Representation/feature learning is a broader term for techniques that allow an ML system to automatically discover representations needed for further tasks such as feature detection and classification. These tasks require inputs that are mathematically easy to represent and convenient to process and do operations with, however real world data is often the opposite (images, videos, sensor data, etc...). These representations are usually continuous vectors.

#### Autoencoders

An autoencoder is a type of neural network architecture that is having three core components: the encoder, the decoder, and the latent-space representation. The encoder compresses the input to a lower latent-space representation via weight matrixes biases and a nonlinear activation function and then the decoder reconstructs it.

#### Prototype learning

Prototype learning is a family of methods where a model represents each class, cluster, or concept using prototypes—vectors in a learned feature space that act as representative examples. A model then makes predictions by comparing new samples to these prototypes. These prototypes provide an abstract representation for many natural categories and concepts.



### K-means Clustering

Clustering is an unsupervised machine learning technique that learns patterns from the features of a dataset without using labels. *K-means* is a simple algorithm that attempts to partition samples into $K$ groups with roughly equal variance, although the latter is not guaranteed.

#### Algorithm in a nutshell

K-means clustering is a prototype learning algorithm used to categorize $N$ (usually vectors of unlabelled data) items into $K$ number of clusters. It is useful for identifying natural groupings of data and structuring raw data. Determining what $K$ should be is important for the segmentation to be meaningful and there are multiple methods for determining the optimal value. K-means has a variety of uses due to its simplicity and effectiveness, including data segmentation, image compression and anomaly detection.

* Unsupervised machine learning technique
* "K-means" = "K averages" / "K central tendencies" / "K centroids"
* Can be looked at as a prototyping tool that discretizes the data into $K$ prototypes

The algorithm will categorize the inputs into $K$ clusters based on similarity. Measuring of similarity is done with the square of the Euclidean distance of the data vectors. Summarized, the iterative algorithm works as follows:

1. **Initialization:** Randomly select $K$ number of centerpoints for the clusters, called centroids.
2. **Assignment:** Each data point is assigned to a cluster based on which centroid is the nearest to it (using Euclidean distance).
3. **Update:** We recalculate the position of each centroid based on the average of the data points in each cluster.
4. **Repeat:** We repeat this process until the position of each centroid is unchanged or we reach a pre-defined iteration limit.

#### Additional details

According to the @scikit_learn_doc, the K-means algorithm "divides a set of $N$ samples $X$ into $K$ disjoint clusters $C$, each described by the mean $\mu_j$ of the samples in the cluster". These means are referred to as *centroids*, and despite living in the same space, they are usually not part of the dataset. The number $K$ is required to be initialized, along with the starting positions of the centroids. The algorithm has an objective function that minimizes the criterion of *within-cluster sum-of-squares*, the so-called *inertia*.

$$
\sum^n_{i=0} \min_{\mu_j \in C} (|| x_i - \mu_j ||^2)
$$

Inertia is "a measure of how internally coherent clusters are". It assumes that clusters are convex and isotropic. As the documentation highlights it, it suffers from drawbacks:

- performs poorly on elongated clusters or manifolds with irregular shapes
- not normalized: lower values are better, zero is optimal, yet in higher-dimensions distances get inflated

![K-means behavior illustrated on simple synthetic datasets with different shapes. Figure by @scikit_learn_clustering_doc.](images/kmeans_behavior_01.png){#fig-background-03}

![K-means behavior illustrated on simple synthetic datasets with different shapes. Figure by @scikit_learn_clustering_doc.](images/kmeans_behavior_02.png){#fig-background-04}

The documentation highlights yet another weakness of the algorithm at the stage of centroid initialization, as written, "K-means will always converge, however this may be to a local minimum". This outcome depends on the initial positions of the centroids, which if assigned randomly, may not find every cluster in the dataset. To address this, the algorithm in the *sklearn* library can be instructed to use a more intelligent initialization technique that places centroids distant from each other and can be used via the $init=\text{"k-means++"}$ argument value. Besides the $n\_init$ parameter can be used to enhance this process, which is the "number of times the k-means algorithm is run with different centroid seeds" where the candidate with the best inertia is kept.



### Neural Networks

In order to comprehend the function of deep learning in unsupervised tasks, it is necessary to first define a neural network in its broadest definition. Neural networks, which are built to imitate biological processes, are fundamentally the engines for modern artificial intelligence. A neural network is described by @aws_neural_network as a "method in artificial intelligence that teaches computers to process data in a way that is inspired by the human brain."

#### Layered structure

According to @gfg_ann_layers, *Artificial Neural Network (ANN)* layers can be explained as follows:

1. **Input layer:** This is the network's entry point. It transfers raw data—such as an image's pixels or a dataset's columns—to the layers that follow. Key role: this layer merely buffers and sends the input signals to the buried levels; no calculation takes place here.
2. **Hidden layers:** The real "learning" and processing takes place in these layers, which are situated between the input and output layers. A network is referred to as "Deep Learning" if it has one or more hidden layers. Key role: they use mathematical processes (weighted sums) and activation functions (like *sigma* or *ReLU*) to add non-linearity to extract features and patterns from the data.
3. **Output layer:** Predictions, output scores.

The *hidden layer* types can vary depending on the architecture:

* **Dense (Fully Connected) layers:** Every neuron connects to every neuron in the next layer
* **Convolutional (Convergent) layers:** Mostly used to identify geometric sequence in images
* **Recurrent (Repetitive) layers:** Applied to sequence data, such as text or periods of time
* **Pooling & Dropout layers:** Used to avoid overfitting and minimize dimensions, respectively

![A diagram of a feedforward artificial neural network illustrating the flow of data through input, hidden, and output layers. Figure by @medium_nn101_part1.](./images/ann.png){#fig-background-05 width=80%}

*AWS* states that "individual nodes can be simple, but when connected in a dense network, they solve complex problems." Supervised learning, in which the network is trained on labeled data (e.g., displaying computer images labeled "cat" until it learns to identify a cat), is traditionally the most well-known use of this design. The data isn't always labeled, though. This leads us to unsupervised techniques such as clustering.

#### Neural Networks in clustering

Clustering is a key method in unsupervised learning, according to @ibm_clustering. It involves "identifying distinct groups of data points" in which the machine must independently recognize patterns without the need for human assistance or pre-existing classifications. According to *IBM*, "clustering algorithms identify distinct groups of data points... such that data points in the same group are more similar to other data points in the same group than those in other groups."

A shift in architecture is necessary to combine these two concepts—by utilizing a Neural Network, which is typically supervised, with Clustering, that is unsupervised. How does the network learn if there are no labels to fix it? Standard feed-forward networks aren't ideally suited for this, as discussed in the @stackexchange_ann_clustering. Rather, certain designs are needed. The usage of automatic encoders is one popular strategy that was brought up in the conversation. One contributor points out that clustering can be accomplished by "training an autoencoder... and then clustering the data in the bottom layer." In this case, the clustering process is far more effective than attempting to group raw, high-dimensional data since the neural network learns to compress data (dimensionality reduction) into a dense representation.

However, there is a more direct "neural" approach to clustering that relies on competitive learning instead of error correction which is *Self-Organizing Map*.

### Self-Organizing Maps (SOM)

According to @geeksforgeeks_som, a *Self-Organizing Map (SOM)* is an unsupervised neural network algorithm that maps high-dimensional data to a lower dimensional grid making interpretation and visualization of the data easier. It follows both competitive and collaborative learning to fit a dataset using neighborhood defining parameters and functions to not only update the best matching unit of a given input data but also its neighborhood with a decay over a certain number of epochs. The learning process is performed in the following steps:

1. **Setup:** The grid of the SOM is defined by its dimensions and hyperparameters such as the neighborhood settings.
2. **Initialization:** The weights (the positions in the $N$-dimensional space) of the grid nodes are initialized randomly.
3. **Competition:** For a given sample, the distance between each node in the grid is calculated and the closest is designated as the winning, that is the *best matching unit (BMU)*.
4. **Collaboration:** The weight vector of the BMU is shifted towards the weight vector of the sample data using the learning rate parameter. The topological neighbors of the BMU are also updated according to the neighborhood function.
5. **Decay:** Over the epochs, the neighborhood and learning rate decay. In the beginning, larger updates are made to help organization of the grid, but later only smaller refinements are performed to finalize convergence to local patterns.
6. **Stopping:** The learning finishes when the maximum number of epochs is reached or when early stopping is instructed.



\newpage
## Methodology

1. Use the *K-means* algorithm to cluster data points in their original $N$-dimensional space. Use the cluster assignments as membership predictions. Visualize the identified clusters in $2D$ feature plots, pairwise. Interpret the outputs, take notes of any strange patterns caused by the dimensionality collapse, if there's any. Visually compare the clustering against true labels. Evaluate the quality of clustering with the help of supervised and unsupervised metrics.

2. Use *Self-Organizing Maps (SOM)* to learn a representation of the dataset that can be visualized in two dimensions through the organized and flattened grid of the map.

3. Segment the learned representation via *Agglomerative Clustering*. Visually compare the clustering against true labels. Evaluate the quality of clustering with the help of supervised and unsupervised metrics.

4. Compare the clustering results of *K-means* and *Self-Organizing Maps*.

5. Briefly introduce a better visualization of the flattened two-dimensional *Self-Organizing Map* visual.



## Experiments

The following experiments were conducted based on the methodology. Performance of each model was evaluated and summarized in @tbl-performance.

1. Clustering via *K-means*
2. Representation learning via *Self-Organizing Map (SOM)*
   * Manual hyperparameter selection
   * Optimized hyperparameter selection
3. Clustering via segmenting the *Self-Organizing Map (SOM)* representation

### Dataset

The `Palmer penguins` dataset was used during the experiments obtained from the `palmerpenguins` python library. The original shape of the dataset is 344 observations and 8 features, including both categorical and numerical.

### Preprocessing

The dataset was limited to the 4 numerical features of `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm` and `body_mass_g`. Observations with missing values in these features were dropped, resulting in a total of 342 records. The features of this dataset were standardized.

### Hyperparameters

For the *K-means* algorithm, $K$ was specified as 3, implied by the three penguin species that we aim to separate. As for the SOM, three set of hyperparameters were tested. The first was defined manually as:

* Grid size: $4 \times 37$
* Neighborhood radius: $\sigma = \frac{max(d_1, d_2)}{3} \simeq 12.3$
* Epochs: $100$
* Learning rate: $\alpha = 0.5$
* Topology: "rectangular",

while the second as:

* Grid size: $12 \times 12$
* Neighborhood radius: $\sigma = \frac{max(d_1, d_2)}{3} = 4$
* Epochs: $100$
* Learning rate: $\alpha = 0.5$
* Topology: "rectangular",

finally, the third one was optimized for the downstream task of *agglomerative clustering* using *ward* linkage with the objective function to maximize the *silhouette score* for 3 target clusters, as follows:

* Grid size: $13 \times 13$
* Neighborhood radius: $\sigma = \frac{max(d_1, d_2)}{3} = 4.33$
* Epochs: $39$
* Learning rate: $\alpha = 0.6$
* Topology: "rectangular"

### Experiment 1: K-means clustering

The `KMeans` estimator from the `sklearn` library was used to fit and cluster the data in its preprocessed four-dimensional space with a heuristic to seek 3 groups. The identified clusters were plotted in two-dimensional scatter plots, for all features, pairwise as depicted in @fig-kmeans-01.

One important insight is that the identified clusters do not separate clearly and there are regions where they overlap each other in the two-dimensional plots.

*Given the K-means algorithm segments the data space, why there are no clear separation lines and how come there are intermingling regions in the resulting plots?*

- **K-Means assumptions**: K-Means divides the data space into convex spherical clusters, that may lead to arbitrary separation lines that do not align with irregularly shaped clusters
- Dataset itself **may not have clear separation** lines for the real groups: these borders can appear intermingled which can be amplified by the styling properties of the scatter plot such as the size of the marker
- **Projection** from $4D$ to $2D$ **distorts** the **original structure**: those regions that appear as intermingled in the collapsed space do not overlap in the original $N$-dimensional feature space

Comparison of the identified clusters against ground truth labels are depicted in @fig-kmeans-02.

![Clusters identified via the *K-means* algorithm.](./images/experiments/kmeans_clustering_01.png){#fig-kmeans-01 width=85%}

::: {#fig-kmeans-02 width=85%}

![](./images/experiments/kmeans_clustering_02_01.png){#fig-kmeans-02-01}

![](./images/experiments/kmeans_clustering_02_02.png){#fig-kmeans-02-02}

![](./images/experiments/kmeans_clustering_02_03.png){#fig-kmeans-02-03}

Comparison of identified clusters and ground truth labels of penguin species. Only 3 pairwise plots out of the 6 are included here.
:::


\newpage
### Experiment 2: Representation learning via Self-Organizing Map

The `MiniSom` class from the `minisom` library was used to learn the representation of the dataset.

First the dataset was temporarily limited to two features and was treated as it had arbitrary dimensionality. The fitted map exhibited a topographic error of less than 1% (0.3%) confirming that the map is mathematically smooth and has effectively preserved the neighborhood relationships of the original high-dimensional data. Knowing this, we can visualize the flattened map of the SOM, the so-called two-dimensional U-matrix that displays the SOM grid with colors according to the average distance of the neighbors for each node represented as pixels, as depicted in @fig-som-learning-01-01. Although the quantization error is also small, being 0.39, considering the standardized feature space with small dimensionality, this suggest that the map itself does not hug the data points closely as @fig-som-learning-01-02 verifies this. Rather, with this configuration, the map behaves like a "principal curve" or a thick snake, winding through the center of the data density.

![](./images/experiments/som_representation_learning_01_01.png){#fig-som-learning-01-01 width=95%}

![](./images/experiments/som_representation_learning_01_02.png){#fig-som-learning-01-02 width=80%}

Returning to the complete, preprocessed dataset with four features, another SOM was fitted exhibiting a topographic error of around 1% (1.2%) and a quantization error of 0.453. The flattened map was obtained as displayed in @fig-som-learning-02-01. Once the dimensionality is no longer 2, it becomes challenging to visualize the convergence of the nodes. When the dataset is treated as an $N$-dimensional dataset, we can use *PCA* to get a glimps on the convergence considering 3 principal component plots, as depicted in @fig-som-learning-02-02 and @fig-som-learning-02-03. In this case, the hidden 4th dimension explains so little variance that the visual provides an accurate state of the convergence, but this becomes less likely as the dimensionality of the dataset increases and the more irregularly the data points are clustered.

After optimizing the hyperparameters, an SOM with topographic error of around 2% (1.8%) and a quantization error of 0.451 was obtained. This is depicted in @fig-som-learning-03-01, @fig-som-learning-03-02 and @fig-som-learning-03-03.

![](./images/experiments/som_representation_learning_02_01.png){#fig-som-learning-02-01 width=95%}

![](./images/experiments/som_representation_learning_02_02.png){#fig-som-learning-02-02}

![](./images/experiments/som_representation_learning_02_03.png){#fig-som-learning-02-03 width=80%}

![](./images/experiments/som_representation_learning_03_01.png){#fig-som-learning-03-01 width=95%}

![](./images/experiments/som_representation_learning_03_02.png){#fig-som-learning-03-02}

![](./images/experiments/som_representation_learning_03_03.png){#fig-som-learning-03-03 width=80%}


\newpage
### Experiment 3: Clustering via segmenting the Self-Organizing Map representation

The SOM representation itself discretizes the dataset yet if non-linear patterns are to be captured a larger, more flexible grid is required, by which it is expected that a real cluster will be covered by multiple nodes of the SOM grid. Therefore, we may apply *Agglomerative Clustering* on the learned representation to find clusters in it on a higher level. These cluster labels, obtained through the best-matching unit (BMU), then can be used for each data point, as depicted in @fig-som-clustering-01-01 and @fig-som-clustering-02-01 for the SOM with the manually selected and optimized hyperparameters, respectively.

![](./images/experiments/som_clustering_01_01.png){#fig-som-clustering-01-01}

![](./images/experiments/som_clustering_02_01.png){#fig-som-clustering-02-01}


### Performance evaluation

The SOM-based clustering outperformed K-means clustering by finding irregularly shaped clusters that aligned better with the real groups of the dataset. The optimized version further improved the performance.

| Experiment                       | Homogeneity     | Completeness     | V-measure     | Accuracy     |
|:---------------------------------|----------------:|-----------------:|--------------:|-------------:|
| K-means clustering               |           0.798 |            0.773 |         0.786 |        0.915 |
| SOM-based clustering             |           0.896 |            0.906 |         0.901 |        0.974 |
| SOM-based (opt.) clustering      |       **0.920** |        **0.936** |     **0.928** |    **0.980** |

: Performance evaluation of the clustering approaches. {#tbl-performance}


### Enhancement of the SOM visual

Since the traditional visuals of the SOM can sometimes be challenging to interpret, and cross referencing the two separate maps may be error prone, an enhanced SOM visual is created using the `lilypond` library (@balogh_lilypond), which provides a more intuitive way to visualize the SOM representation. The traditional approach is displayed in @fig-som-enhancement-01, while the enhanced version is depicted in @fig-som-enhancement-02-01.

Besides, an additional layer is created in @fig-som-enhancement-02-02, to visualize the edges between $1st$ and $2nd$ BMU pairs implied by the data points: all connections and only those violating the neighborhood constraint, in each subplot, respectively.

![](./images/experiments/som_visual_enhancement_01.png){#fig-som-enhancement-01}

![](./images/experiments/som_visual_enhancement_02_01.png){#fig-som-enhancement-02-01}

![](./images/experiments/som_visual_enhancement_02_02.png){#fig-som-enhancement-02-02}

\newpage