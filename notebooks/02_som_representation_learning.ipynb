{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c61974e1",
   "metadata": {},
   "source": [
    "# Visualizing a representation of the `penguins` dataset in 2D with *Self-Organizing Map*\n",
    "\n",
    "In [01_kmeans_clustering.ipynb](./01_kmeans_clustering.ipynb) we use/used the `K-Means` algorithm to find groups of data points. Here, we want to learn a representation of the dataset. K-Means may also be used for representation learning in a sense that it discretizes the data space into disjoint regions. However, once we have a dataset that has more than three dimensions, visualizing these groups becomes challenging.\n",
    "\n",
    "On the other hand, a *Self-Organizing Map* is similar to `K-Means` in a sense that it also operates with centroids to which data samples get mapped, yet one very important difference here is that the centroids are chained together from the very beginning so after it discovers and maps the dataset, the neighboring information can be used to flatten the SOM grid into a 2D visual. As hinted, SOM itself could be used for clustering as well, yet in this notebook the objective is to learn and visualize the simpler representation of the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4602cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RAND = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc490a",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8abaded",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = pd.read_csv(\"../datasets/penguins/simple/X_scaled.csv\", index_col=0, header=0)\n",
    "y = pd.read_csv(\"../datasets/penguins/simple/y.csv\", index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cc1849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Do some exploratory data analysis (Optional)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e8a696",
   "metadata": {},
   "source": [
    "## Experiment 1: Use only two features\n",
    "\n",
    "In this experiment, we treat the first two features of the dataset as it was a dataset with arbitrary dimensions and use SOM to learn the representation of it and to visualize it in 2D. In this case we will be able to verify the converge easily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1394e381",
   "metadata": {},
   "source": [
    "1. Define hyperparameters\n",
    "\n",
    "\t> use `calc_recommended_grid_size` methods to get candidate values for `d1` and `d2`\n",
    "\n",
    "\t> e.g.: `print(\"Recommended total node count and dimension size for SOM:\", calc_recommended_grid_size(...))`\n",
    "\n",
    "2. Fit SOM\n",
    "\n",
    "\t> use `train_batch_som` method to obtain an `som` object fitted on the data\n",
    "\n",
    "\t> observe the quality of the SOM outputted, and experiment with different hyperparameter combinations to arrive at a better representation\n",
    "\n",
    "\t> *Quantization error (QE)* measures the Mean Residual from best-matching nodes aka. nearest centroid\n",
    "\n",
    "\t> *Topographic error (TE)* measures the proportion of data points for which the 1st and 2nd best-matching units are not neighbors in the SOM grid\n",
    "\n",
    "3. Extract learned node weights (prototype / representative point positions) from SOM\n",
    "\n",
    "\t> use `som.get_weights()` and flatten them\n",
    "\n",
    "4. Visualize these 2D coordinates on the 2D dataset\n",
    "\n",
    "\tWe may expect to end up with a similar results but could be better:\n",
    "\n",
    "\t![](./_images/som_representation_expectation.png)\n",
    "\n",
    "5. Visualize the representation via the SOM maps\n",
    "\n",
    "\tWe may expect to end up with a similar results but could be better:\n",
    "\n",
    "\t![](./_images/som_representation_maps.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2716d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from _utilities.som import calc_recommended_grid_size, train_som\n",
    "from _utilities.som_plot import visualize_distance_map, visualize_hitmap, place_node_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073cf86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = X_scaled.columns[:2]\n",
    "print(\"Selected features:\", selected_columns.tolist())\n",
    "\n",
    "X_scaled_F12 = X_scaled[selected_columns].copy().values\n",
    "print(\"Data shape:\", X_scaled_F12.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccb3d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Hyperparameters\n",
    "\n",
    "hyparams = {\n",
    "    \"d1\": ?,\n",
    "    \"d2\": ?,\n",
    "    \"sigma\": ?, # start with D/2 for more robust map, or D/3 or smaller for more flexible\n",
    "    \"learning_rate\": .5, # start with .5, usually between .5-1\n",
    "    \"num_iteration\": 10, # a single iteration touches every data point in the dataset\n",
    "    \"topology\": \"rectangular\"\n",
    "}\n",
    "\n",
    "# TODO: Fit SOM\n",
    "\n",
    "som = train_som(X=?, **hyparams, random_seed=RAND, verb=True)\n",
    "\n",
    "# TODO: Extract learned node weights\n",
    "\n",
    "node_weights = som.get_weights()\n",
    "node_weights_flat = node_weights.reshape(-1, node_weights.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08268329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# data points\n",
    "plt.scatter(?, ?, color=\"cornflowerblue\", edgecolors=\"royalblue\", alpha=.75)\n",
    "\n",
    "# node positions\n",
    "plt.scatter(node_weights_flat[:, 0], node_weights_flat[:, 1], color=\"red\", edgecolors=\"red\", alpha=.9)\n",
    "place_node_edges(som, ax=plt.gca())\n",
    "\n",
    "plt.title(?)\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.xlabel(?)\n",
    "plt.ylabel(?)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16937836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize with the SOM maps\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "visualize_distance_map(som=?, X=?, ax=ax0)\n",
    "visualize_hitmap(som=?, X=?, ax=ax1)\n",
    "\n",
    "plt.tight_layout()         \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405fc8b9",
   "metadata": {},
   "source": [
    "## Experiment 2: Use all the 4 features\n",
    "\n",
    "Follow the same steps yet use the original dataset (`X_scaled`) not the limited one. The node positions can't be visualized in two or three dimensional plots. Either we plot the features pairwise, 3D, or use PCA to reduce dimensions. But we expect that the final SOM maps can capture the original high-dimensional structure and place similar items in the original space onto nearby locations in the SOM map. Compare the final results of the two scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7f9d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Follow the same steps but for the original dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dscvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
